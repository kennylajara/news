# AI/ML Directory - News Embeddings & Training

Este directorio contiene todo lo relacionado con machine learning y entrenamiento de modelos de embeddings para el proyecto de noticias.

## üìÅ Estructura del Directorio

```
ai/
‚îú‚îÄ‚îÄ README.md                    # Este archivo
‚îú‚îÄ‚îÄ corpus/                      # Base de datos de corpus (texto plano)
‚îÇ   ‚îî‚îÄ‚îÄ raw_news.db              # SQLite con art√≠culos en texto plano
‚îú‚îÄ‚îÄ models/                      # Modelos entrenados
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/              # Modelos de embeddings
‚îÇ       ‚îî‚îÄ‚îÄ news-embeddings-*-TIMESTAMP/
‚îî‚îÄ‚îÄ training/                    # Scripts de entrenamiento
    ‚îú‚îÄ‚îÄ README.md                # Documentaci√≥n detallada
    ‚îú‚îÄ‚îÄ analysis.py              # An√°lisis de distribuci√≥n del corpus
    ‚îú‚îÄ‚îÄ loaders/
    ‚îÇ   ‚îî‚îÄ‚îÄ category_loader.py   # Carga datos por categor√≠a/subcategor√≠a
    ‚îî‚îÄ‚îÄ embeddings/
        ‚îú‚îÄ‚îÄ simple.py            # Entrenamiento jer√°rquico simple
        ‚îî‚îÄ‚îÄ controlled_ratios.py # Entrenamiento con ratios controlados
```

## üéØ Prop√≥sito

Este directorio permite entrenar modelos de embeddings personalizados que entienden mejor el dominio espec√≠fico de noticias en espa√±ol. Los modelos aprenden a:

- Distinguir entre diferentes categor√≠as de noticias (pol√≠tica, deportes, econom√≠a, etc.)
- Reconocer noticias relacionadas dentro de la misma categor√≠a
- Generar representaciones vectoriales optimizadas para b√∫squeda y clustering

## üöÄ Quick Start

### 1. Exportar Corpus

Primero, exporta art√≠culos desde la base de datos principal al corpus:

```bash
# Desde el root del proyecto
uv run news export corpus --limit 500
```

Esto crea `ai/corpus/raw_news.db` con art√≠culos en texto plano, separando categor√≠as y subcategor√≠as.

### 2. Analizar Distribuci√≥n

Verifica que tienes suficientes art√≠culos y categor√≠as:

```bash
python ai/training/analysis.py
```

**Recomendado**: Al menos 100+ art√≠culos con 3+ categor√≠as diferentes.

### 3. Entrenar Modelo

Elige una estrategia de entrenamiento:

```bash
# Opci√≥n 1: Entrenamiento simple (recomendado para empezar)
python ai/training/embeddings/simple.py

# Opci√≥n 2: Entrenamiento con ratios controlados (para optimizar)
python ai/training/embeddings/controlled_ratios.py
```

El modelo se guarda en `ai/models/embeddings/news-embeddings-*-TIMESTAMP/`

### 4. Usar Modelo

```python
from sentence_transformers import SentenceTransformer

# Cargar modelo entrenado
model = SentenceTransformer('ai/models/embeddings/news-embeddings-simple-20251120_141530')

# Generar embeddings
texts = [
    "Presidente anuncia nuevas medidas econ√≥micas",
    "Equipo nacional gana campeonato de b√©isbol"
]
embeddings = model.encode(texts)

# Calcular similitud sem√°ntica
from sentence_transformers.util import cos_sim
similarity = cos_sim(embeddings[0], embeddings[1])
print(f"Similitud: {similarity.item():.4f}")  # Cercano a 0 = diferentes categor√≠as
```

## üìä Corpus Database (`corpus/raw_news.db`)

Base de datos SQLite optimizada para entrenamiento de ML:

### Schema

```sql
CREATE TABLE articles (
    id INTEGER PRIMARY KEY,
    hash TEXT NOT NULL UNIQUE,
    url TEXT NOT NULL,
    source_domain TEXT NOT NULL,
    source_name TEXT NOT NULL,

    title TEXT NOT NULL,
    subtitle TEXT,
    author TEXT,
    published_date TEXT,
    location TEXT,
    content TEXT NOT NULL,          -- ‚≠ê Texto plano (sin markdown)
    category TEXT,                   -- ‚≠ê Categor√≠a principal
    subcategory TEXT,                -- ‚≠ê Subcategor√≠a

    created_at TEXT NOT NULL,
    updated_at TEXT NOT NULL,
    exported_at TEXT NOT NULL
);
```

### Caracter√≠sticas

- ‚úÖ **Texto plano**: Sin formato markdown (sin `**negritas**`, sin `[links](url)`)
- ‚úÖ **Categor√≠as separadas**: `category` y `subcategory` en columnas distintas
- ‚úÖ **√çndices optimizados**: Para b√∫squedas r√°pidas por categor√≠a, fuente, fecha
- ‚úÖ **Actualizaci√≥n inteligente**: Detecta duplicados por hash

### Comandos √ötiles

```bash
# Ver estad√≠sticas del corpus
uv run news export stats

# Exportar m√°s art√≠culos
uv run news export corpus --domain diariolibre.com --limit 1000

# Verificar contenido directamente
sqlite3 ai/corpus/raw_news.db "SELECT category, subcategory, COUNT(*) FROM articles GROUP BY category, subcategory"
```

## ü§ñ Modelos Entrenados (`models/embeddings/`)

Los modelos entrenados se guardan aqu√≠ con timestamps para versionado.

### Convenci√≥n de Nombres

```
news-embeddings-simple-20251120_141530/
news-embeddings-balanced-20251120_153045/
```

- `simple`: Entrenamiento jer√°rquico simple
- `balanced`: Entrenamiento con ratios controlados
- Timestamp: `YYYYMMDD_HHMMSS`

### Contenido del Directorio del Modelo

Cada modelo contiene:
- `config.json` - Configuraci√≥n del modelo
- `pytorch_model.bin` - Pesos del modelo
- `tokenizer_config.json` - Configuraci√≥n del tokenizer
- `vocab.txt` - Vocabulario
- Otros archivos de sentence-transformers

### Cargar Modelo

```python
from sentence_transformers import SentenceTransformer

# Por path absoluto
model = SentenceTransformer('/home/user/news/ai/models/embeddings/news-embeddings-simple-20251120_141530')

# Por path relativo (desde root del proyecto)
model = SentenceTransformer('ai/models/embeddings/news-embeddings-simple-20251120_141530')
```

## üìö Training Scripts (`training/`)

Scripts para entrenar modelos personalizados.

### `analysis.py`

Analiza la distribuci√≥n de art√≠culos por categor√≠a/subcategor√≠a.

**Uso**: `python ai/training/analysis.py`

**Output**:
```
üìÅ Pol√≠tica: 150 noticias
  ‚îú‚îÄ Nacional: 80 noticias
  ‚îú‚îÄ Internacional: 70 noticias

üìÅ Deportes: 100 noticias
  ‚îú‚îÄ B√©isbol: 50 noticias
  ‚îú‚îÄ F√∫tbol: 30 noticias

üìä Total noticias: 250
üìä Pares potenciales (aprox): 1500
```

### `embeddings/simple.py`

Entrenamiento jer√°rquico simple con niveles de similaridad:
- Misma subcategor√≠a: 0.95
- Misma categor√≠a: 0.7
- Diferente categor√≠a: 0.0
- T√≠tulo-contenido: 1.0

**Uso**: `python ai/training/embeddings/simple.py`

### `embeddings/controlled_ratios.py`

Entrenamiento con control fino de ratios entre tipos de pares.

**Uso**: `python ai/training/embeddings/controlled_ratios.py`

**Ventaja**: Permite ajustar balance de pares positivos/negativos.

Ver [`training/README.md`](training/README.md) para documentaci√≥n detallada.

## üîß Configuraci√≥n Avanzada

### Cambiar Modelo Base

Edita los scripts de entrenamiento y cambia:

```python
base_model='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'
```

Opciones recomendadas:
- `paraphrase-multilingual-MiniLM-L12-v2` (default, r√°pido, 118M params)
- `distiluse-base-multilingual-cased-v2` (m√°s peque√±o, 135M params)
- `paraphrase-multilingual-mpnet-base-v2` (m√°s grande, mejor calidad, 278M params)

### Ajustar Hiperpar√°metros

```python
train_embeddings(
    db_path=db_path,
    output_dir=output_dir,
    base_model='...',
    epochs=4,           # M√°s epochs = mejor aprendizaje (pero riesgo overfitting)
    batch_size=16       # M√°s grande = m√°s r√°pido (pero m√°s memoria)
)
```

### GPU vs CPU

Los scripts detectan autom√°ticamente CUDA. Para forzar CPU:

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'
```

## üìà Workflow Completo

```bash
# 1. Poblar base de datos principal desde cach√©
uv run news article fetch-cached --limit 500

# 2. Exportar al corpus (texto plano)
uv run news export corpus

# 3. Verificar distribuci√≥n
python ai/training/analysis.py

# 4. Entrenar modelo
python ai/training/embeddings/simple.py

# 5. Verificar modelo guardado
ls -lh ai/models/embeddings/

# 6. Usar en tu aplicaci√≥n
python -c "from sentence_transformers import SentenceTransformer; \
           model = SentenceTransformer('ai/models/embeddings/news-embeddings-simple-...'); \
           print(model.encode(['Test']))"
```

## üêõ Troubleshooting

### "No training examples generated"
- **Causa**: Corpus vac√≠o o sin categor√≠as
- **Soluci√≥n**: `uv run news export corpus`

### "Database not found"
- **Causa**: `ai/corpus/raw_news.db` no existe
- **Soluci√≥n**: `uv run news export corpus`

### "Only X categories found"
- **Causa**: Pocas categor√≠as para entrenamiento efectivo
- **Soluci√≥n**: Exporta m√°s art√≠culos de diferentes fuentes

### "CUDA out of memory"
- **Causa**: Batch size muy grande para GPU
- **Soluci√≥n**: Reduce `batch_size` a 8 o 4

### Modelo no mejora
- **Causa**: Datos muy similares o distribuci√≥n desbalanceada
- **Soluci√≥n**:
  - Verifica distribuci√≥n con `analysis.py`
  - Exporta m√°s art√≠culos de categor√≠as diferentes
  - Prueba `controlled_ratios.py` con ratios ajustados

## üìñ Recursos Adicionales

- [Sentence-Transformers Documentation](https://www.sbert.net/)
- [Training Overview](https://www.sbert.net/docs/training/overview.html)
- [Cosine Similarity Loss](https://www.sbert.net/docs/package_reference/losses.html#cosinesimilarityloss)

## üéØ Casos de Uso

### 1. B√∫squeda Sem√°ntica

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('ai/models/embeddings/news-embeddings-simple-...')

# Corpus de noticias
corpus = [
    "Presidente anuncia reforma tributaria",
    "Equipo nacional clasifica al mundial",
    "Nuevo presupuesto nacional aprobado"
]

# Query del usuario
query = "impuestos y econom√≠a"

# Generar embeddings
corpus_embeddings = model.encode(corpus)
query_embedding = model.encode(query)

# Buscar m√°s similares
hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=3)

for hit in hits[0]:
    print(f"{corpus[hit['corpus_id']]} (score: {hit['score']:.4f})")
```

### 2. Clustering de Noticias

```python
from sentence_transformers import SentenceTransformer
from sklearn.cluster import KMeans

model = SentenceTransformer('ai/models/embeddings/news-embeddings-simple-...')

# Art√≠culos
articles = [...]
embeddings = model.encode(articles)

# Clustering
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(embeddings)

# Agrupar
from collections import defaultdict
groups = defaultdict(list)
for idx, cluster_id in enumerate(clusters):
    groups[cluster_id].append(articles[idx])
```

### 3. Deduplicaci√≥n

```python
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('ai/models/embeddings/news-embeddings-simple-...')

def find_duplicates(articles, threshold=0.85):
    embeddings = model.encode(articles)
    cos_sim = util.cos_sim(embeddings, embeddings)

    duplicates = []
    for i in range(len(articles)):
        for j in range(i+1, len(articles)):
            if cos_sim[i][j] > threshold:
                duplicates.append((i, j, cos_sim[i][j].item()))

    return duplicates
```

---

**Para documentaci√≥n detallada de entrenamiento**, ver [`training/README.md`](training/README.md)
